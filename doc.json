["<h1>Design and Control of Roller Grasper V2 for In-Hand Manipulation|", "", "<h2>Shenli Yuan", "<s2>1 , 2", "<h2>, Lin Shao", "<s2>2", "<h2>, Connor L. Yako", "<s2>1 , 2", "<h2>, Alex Gruebele", "<s2>1", "<h2>, and J. Kenneth Salisbury", "<s2>2|", "", "<s1>Abstract \u2014 The ability to perform in-hand manipulation| still remains an unsolved problem; having this capability| would allow robots to perform sophisticated tasks requiring| repositioning and reorienting of grasped objects. In this work,| we present a novel non-anthropomorphic robot grasper with| the ability to manipulate objects by means of active surfaces at| the \ufb01ngertips. Active surfaces are achieved by spherical rolling| \ufb01ngertips with two degrees of freedom (DoF) \u2013 a pivoting| motion for surface reorientation \u2013 and a continuous rolling| motion for moving the object. A further DoF is in the base| of each \ufb01nger, allowing the \ufb01ngers to grasp objects over a| range of size and shapes. Instantaneous kinematics was derived| and objects were successfully manipulated both with a custom| handcrafted control scheme as well as one learned through| imitation learning, in simulation and experimentally on the| hardware.|", "", "<p>I. I", "<s2>NTRODUCTION|", "", "<p>In an effort to bring robots from the laboratory into real| world environments, researchers have endeavored to develop| increasingly dexterous robots that can interact deftly with| objects. In order for such robots to take on a wide range| of everyday tasks, they need to be capable of sophisticated| object manipulation. Many vital higher-level tasks will rely| on a robot\u2019s capability to perform in-hand manipulation by| reorienting objects while maintaining the grasp. Out of all| the grasping and manipulation tasks, in-hand manipulation| is among the ones that require the most dexterity.| A background of in-hand manipulation literature is pre-| sented in [1] and a more extensive review of robot hands and| graspers is given in [2]. Common approaches to designing| robotic grippers that can perform in-hand manipulation are:| anthropomorphic hands which take advantage of intrinsic| human dexterity, but due to the high number of degrees| of freedom are complex and expensive [3] [4] [5]; under-| actuated hands which passively conform to objects, achieving| good grasp stability, but at the cost of the controllability| needed to perform many in-hand manipulation tasks [6] [7]| [8] [9]; grippers with active surfaces (such as conveyors)| which allow for the object to be manipulated without chang-| ing grasp pose, but with a \ufb01xed conveyor orientation limiting| possible motions [10] [11] [12].| We developed a novel grasper design using articulated, ac-| tively driven spherical rollers located at the \ufb01ngertips, shown| in Fig. 1. By incorporating continuous rotating mechanisms,|", "", "<s2>*This work has been funded, in part, by the Stanford Interdisciplinary| Graduate Fellowship. Detailed implementations can be found at:  https://| ccrma.stanford.edu/\u02dcshenliy/roller_grasper_v2.html|", "<s5>1", "<s2>Department of Mechanical Engineering, Stanford University|", "<s5>2", "<s2>Stanford Arti\ufb01cial Intelligence Lab (SAIL), Stanford University| { shenliy, lins2, clyako, agruebe2 } @stanford.edu,| jks@cs.stanford.edu|", "<s2>Fig. 1.| The Roller Grasper V2 prototype mounted on a UR-5 robot arm.| Three \ufb01ngers, each with three degrees of freedom, can grasp and reorient| an object using rolling contact with the rubber coated rollers|", "", "<p>it is possible to create graspers that are highly capable but| relatively simple by design. The active surface achieved by| rolling and reorientation of the spherical rollers allow the| grasper to perform in-hand manipulation without the need| for \ufb01nger gaiting. Rolling contact on the object can be| viewed as a motion that continuously breaks contact with| the object while simultaneously re-establishing contact at| adjacent locations. The ability to reorient an object to any| pose also lessens the need to use externally actuated degrees| of freedom (e.g. actuation of the robotic arm and wrist)| which simpli\ufb01es the control scheme. More importantly, the| spherical design of the \ufb01nger tips allows for stable grasps| independent from the roller orientations, eliminating the need| to analyze grasping modes for different combinations of| roller orientations.| Learning robust policies for in-hand manipulation has been| a long-standing challenge in robotics due to the complex-| ity of modelling the object and grasper contacts and the| dif\ufb01culty of controlling \ufb01nger motion in long and compli-| cated manipulation sequences. Deep Reinforcement Learning| (DRL) has been used to learn dexterous manipulation [13]| [14]. However, learning to hold the object \ufb01rmly and stably| while transforming the object with deep reinforcement learn-| ing requires many more training episodes and a carefully| designed reward function. To overcome these issues, we used| an imitation learning based approach to learn a control policy| in order to arbitrarily transform an object while holding it.|", "<p>We demonstrated the effectiveness of this learned policy both| in simulation and in real world experiments.| Our in-hand manipulation system consisted of a 3-\ufb01ngered| grasper with spherical rollers at the \ufb01ngertips, an overhead| RGBD camera, and objects with QR-tags on all faces. A| handcrafted control policy and an imitation learning policy| were developed to perform complex in-hand object trans-| formations. To the best of our knowledge, this work is the| \ufb01rst attempt at developing a grasper with active surfaces| at the \ufb01ngertips that transforms grasped objects through an| imitation learning policy. The paper is structured as follows:| we \ufb01rst discuss our previous iteration of this robotic grasper| as well as other design and algorithmic approaches to robotic| grasping/in-hand manipulation (Section II). Section III then| brie\ufb02y describes the hardware. Section IV discusses the| formulation of the handcrafted control policy as well as| the imitation learning approach. The paper then provides an| overview of the experimental setup in simulation and in real| life, and concludes by reporting and discussing the results| (Section V and Section VI).|", "<p>II. R", "<s2>ELATED", "<p> W", "<s2>ORK|", "", "<p>A. Previous Grasper Design|", "<p>Our previous work [1] used articulated, actively driven| cylindrical rollers at the \ufb01ngertips of a grasper to explore| imparting motion within a grasp using active surfaces. The| grasper used three modular 3-DoF \ufb01ngers, and demonstrated| full 6-DoF spatial manipulation of objects including a sphere,| cube, and cylinder, as well as various grasping modalities.| One limitation of the previous roller grasper is the grasp| stability. Due to the cylindrical design of the \ufb01nger tips,| several grasping con\ufb01gurations are unstable, resulting in| undetermined manipulation behaviors. The redundant com-| binations of grasping con\ufb01gurations also complicates the| control scheme as the con\ufb01guration used is dependent on| speci\ufb01c manipulation tasks and the object being manipulated.|", "<p>B. In-Hand Manipulation|", "<p>In-hand manipulation is an age-old question in robotics| with a rich literature, from two-\ufb01ngered grippers [15] [16],| to dexterous hands [13]. We brie\ufb02y review the relevant in-| hand manipulation works in this subsection. To achieve in-| hand manipulation, multi-\ufb01ngered dexterous hands utilize the| redundancy of the \ufb01ngers to move the object while holding| it. Under-actuated hands are able to leverage model based| control [17] for in-hand manipulation tasks. There are also| task-speci\ufb01c designs of under-actuated hands which enable| a limited set of repositioning skills [18] [19].| Other approaches to in-hand manipulation have been| explored which rely on gravity with controlled slip [20]| [21], induced accelerations [22] [23], or the environment| [15] [24] [25] to reduce the dexterity required of the hand.| However, such approaches require complex control and mod-| eling schemes or dependency on available environmental| geometry.| Contrary to modeling the complex dynamics involved in| grasping and object reorientation, some researchers have|", "<p>opted to use reinforcement learning (RL) to search for| optimal policies. This is especially useful when using un-| deractuated graspers or graspers with high DoF\u2019s. In [26] an| underactuated grasper with tactile sensors on the \ufb01ngertips| was used to horizontally slide a wooden cylinder back and| forth by rolling it along each \ufb01nger. The learned policy was| evaluated on cylinders of different masses, sizes, and friction| coef\ufb01cients. They found that the policy performed better than| a hard-coded control policy, which was used as a baseline,| but still struggled with cylinders with low-friction properties.| DRL has also been implemented successfully on the 24 DoF| Shadow Hand for dynamically moving a cylinder in hand and| for arbitrarily reorientating a cube using visual information| [13] [14]. However, our grasper needs to maintain hold of| the object solely through friction at the roller contact during| the manipulation process. This means that a tiny perturbation| of the \ufb01ngertip could possibly break the contact and lead to| a dropped object. Therefore, the space of successful policies| is incredibly small relative to the entire policy space. Since| exploration is necessary in any DRL problem, it is dif\ufb01cult| for the algorithm to converge to the optimal policy. To| avoid this problem, we instead adopted an imitation learning| method, which will be discussed in the next section.|", "<p>C. Imitation Learning Methods|", "<p>Imitation learning aims to learn control policies by ob-| serving expert demonstrations. There are in general two| types of approaches to tackle an imitation learning problem.| Behaviour cloning  aims to train the agent to learn a mapping| from observations to actions given demonstrations, in a| supervised learning fashion [27] [28]. Another approach is| Inverse Reinforcement Learning  [29] [ ? ], which attempts to| learn a reward function that describes the given demonstra-| tions. Our method falls under the scope of  Behavior Cloning| which has led to many successes in robotics [31] [32]; our| approach is based on one  Behaviour Cloning  method called| DAgger [28]. To tackle the problem of generating expert| demonstrations, we also develop a method to accumulate| the demonstration examples iteratively starting from a few| expert demonstrations.|", "<p>III. D", "<s2>ESIGN|", "", "<p>A. Hardware Design|", "<p>The gripper (Fig. 2(a)) consists of three \ufb01ngers, each| having three degrees of freedom (DoF). The \ufb01rst DoF is| at the base of each \ufb01nger and consists of a revolute joint| directly driven by a Robotis Dynamixel XM430-W350 smart| actuator. The other two DoF are located at each \ufb01ngertip,| and are responsible for steering and rolling. The second| joint shown in Fig. 2(a) is orthogonal to the \ufb01rst DoF, and| is driven by a micro DC motor with built-in gearbox and| quadrature encoder (Servocity No.638099). For a compact| form-factor, this actuator is located remotely from the axis| of rotation through a timing belt (Fig. 2(b)), and allows the| roller assembly to be pitched up to 180 degrees. The \ufb01nal| DoF is actuated using the same type of geared motor but| housed inside the roller assembly (Fig. 2(c)), allowing it to|", "", "<s2>Fig. 2.| A CAD model of the grasper: (a) the three degrees of freedom each \ufb01nger has (b) an exploded view of each \ufb01nger (c) an exploded view of the| roller assembly that contacts the object being manipulated|", "<s2>TABLE I| P", "<s4>HYSICAL", "<s2> P", "<s4>ROPERTIES|", "", "<s2>Property| Value|", "<s2>Link  a  (referred in Fig. 4)| 48 mm|", "<s2>Link  b  (referred in Fig. 4)| 122 mm|", "<s2>Link  r  (referred in Fig. 4)| 21 . 5 mm|", "<s2>Whole grasper weight| 700 g|", "<s2>Maximum normal force at the \ufb01ngertip| 33 . 6 N|", "<s2>Maximum roller shear force| 16 . 4 N|", "", "<p>perform continuous rotation of the spherical contact surface| without its cables winding. The roller is encased in a pair of| 2mm-thick semi-spherical silicone covers to provide a high-| friction surface for grasping and manipulation. The silicone| covers are molded from Mold Star 16 Fast (SmoothOn),| cast in a 2-part 3D printed mold (Objet Vero white, glossy| surface \ufb01nish), with a coat of Ease Release 2000 release| agent. They are adhered to the 3D printed roller surfaces| using silicone compatible tape (McMaster No.7213A31) The| reference frame of the grasper is shown in Fig. 4 and the key| physical parameters are listed in Table I.| The design allows for unbounded reorientation of the| grasped object, while the range of translation is determined| by various factors such as the physical dimensions, shape,| in-hand orientation, mass, and resulting friction coef\ufb01cient| of the grasped objects.|", "<p>B. System Architecture|", "<p>The system architecture used to operate the gripper is| shown in Fig. 3. A high-level API was developed to inter-| face between the low-level (hardware) information and the| manipulation algorithm. Information transferred during the| bidirectional communication includes positions for each joint| of the \ufb01ngers, the current limit of the base joint, as well as| the control parameters for controlling the motors. Current to| the Dynamixel motors is used to set the stiffness of the base| joints and estimate the force exerted by the object on each| \ufb01nger during manipulation.| A Teensy 3.6 microcontroller is used to handle communi-| cation with the high-level API as well as low-level control|", "", "<s2>Fig. 3.| System architecture|", "", "<p>of the motors. The six micro gearmotors located at the| intermediate joints and within the rollers are controlled by| PD position controllers handled by the microcontroller. The| Dynamixel motors each run local PD control and commu-| nicate with the Teensy microcontroller through a TTL half-| duplex asynchronous serial communication protocol.|", "<p>IV. T", "<s2>ECHNICAL", "<p> A", "<s2>PPROACH|", "", "<p>In this section we begin by describing the analytical| approach that led to development and implementation of a| handcrafted control policy. We then discuss how this control| policy was \ufb01ne-tuned based on simulation results in order| to act as an expert trajectory generator for the imitation| learning. The \ufb01nal subsection describes the imitation learning| formulation.| Initially, several DRL methods were used in place of imita-| tion learning. From these preliminary experiments we learned| that the space of successful policies, not necessarily optimal,| was especially small. This becomes a critical problem in| DRL given large policy spaces since signi\ufb01cant exploration| is necessary. Instead, we elected to pursue imitation learning| which proved to be both effective and ef\ufb01cient.|", "<p>A. Analysis|", "<p>Manipulating an object through rolling contact can be| viewed as navigating the rollers on the object. Therefore,| an object transformation can be achieved by navigating the| rollers from their initial contact locations to the desired \ufb01nal| contact locations. While there is no unique solution for the| paths the rollers take during their navigation for given initial| and \ufb01nal grasping poses, it is possible to solve, on this|", "", "<s2>Fig. 4.| Reference frames of each of the three joints within each| \ufb01nger, relative to the base coordinate system. Dashed lines indicate neutral| positions from which joint angles are measured.|", "", "<p>non-holonomic system, for the instantaneous joint velocities| based on known object geometry, object pose, object veloc-| ity, and contact conditions (rolling without slipping).| Object geometry and pose are necessary to calculate the| contact locations, which determine the relationship between| joint motions and given contact motions on the rollers (the| contact jacobian matrix). This information can subsequently| be used to map the desired object motion to the motions| at the contact point on the object. Applying the contact| condition of rolling without slipping means that the contact| velocity on the object and the contact velocity on the roller| are equal. Thus, inverse kinematics can be used to calculate| the joint velocities required for a desired object motion, a| process similar to the calculation of geometric stiffness for| grasping presented in [33].| The problem is formulated as follows: given the object\u2019s| initial and target position and orientation, compute the con-| tact location and contact motion on the roller, and then| compute the pivot joint orientation and motions of the base| and roller joints.| Given the desired motion of the object and known contact| locations, we can obtain the contact motion by|", "", "<s2>Fig. 5.| Contact motion breaks down to two components|", "", "<p>\u03b4x", "<s3>contact", "<p> =  J", "<s3>obj", "<p>\u03b4x", "<s3>obj|", "<p>(1)|", "<p>where  J", "<s3>obj", "<p> is the jacobian matrix mapping object motion to| the motion at the contact frame.| On the other hand, with the contact motion  \u03b4x", "<s3>contact", "<p>, the| object position  x", "<s3>obj", "<p>, and roller position  x", "<s3>roller", "<p> all known,| the contact motion can be interpreted as the motion at the| contact location due to the movement of the 3 joints:|", "<p>\u03b4x", "<s3>contact", "<p> =  J", "<s3>\u03b8", "<p>\u03b4\u03b8| (2)|", "<p>Where  J", "<s3>\u03b8", "<p> is the contact jacobian matrix for mapping \ufb01nger| joint motions to motions in the contact coordinates, and| \u03b4\u03b8  is a vector of joint motions of the \ufb01nger. In many| robotics applications  \u03b4\u03b8  is determined directly by solving| (2). However, this method is not particularly applicable to| this gripper design for the following reasons:|", "<p>1) The contact locations are very close to the pivot joint| so the \ufb01nger is always close to a singularity.|", "<p>2) The pivot joint has a joint limit of  [ \u2212", "<s3>\u03c0|", "<s3>2", "<p>,", "<s3> \u03c0|", "<s3>2", "<p>]  meaning| that in many cases the instantaneous velocity required| from the pivot joint to achieve the desired contact| velocity cannot be reached.| Therefore, instead of carrying out the full inverse kine-| matics, we divide the contact motion into two components:| the component resulting from the motion of the base joint,| \u03b4x", "<s3>cb", "<p>, and the component resulting from the rolling,  \u03b4x", "<s3>cr", "<p>.| The pivot joint is used to reorient the roller so that the| rolling direction at the contact point is aligned with  \u03b4x", "<s3>cr", "<p>.| This approximation is suf\ufb01cient because (1) when the contact| locations are close to the pivot axis, the pivot motion does| not have a signi\ufb01cant impact on the object motion, and (2)| the object being grasped is over constrained by the three| \ufb01ngers, so using the soft \ufb01nger model [34] to approximate| roller contacts, the torque exerted by pivoting is compensated| by the lateral frictions from the two other \ufb01ngers. Thus, the| singularity is advantageous in that it enables sole use of the| pivot joint to align the roller in order to achieve  \u03b4x", "<s3>cr", "<p>.| We developed a handcrafted control strategy based on| the above formulation with some modi\ufb01cations that \ufb01t the| gripper design. The handcrafted control policy tested in| simulation and on the hardware assumes that the object| geometry is spherical with radius  R . Speci\ufb01cally, the below| calculations are for a single \ufb01nger, but can be applied to| the two other \ufb01ngers as well. For the simplicity of notation,| we use subscript  1 ,  2 , and  3  to represent the base joint,| pivot joint, and roller joint, respectively, and leave out the| subscript  A ,  B  or  C  because the following derivation would| be identical for each \ufb01nger.| In order to obtain  \u03b4x", "<s3>cb", "<p> and  \u03b4x", "<s3>cr", "<p>, we need to project| \u03b4x", "<s3>contact", "<p> onto  Z", "<s3>2", "<p> and the contact plane (Fig. 5). The contact| plane can be described by its normal vector  \u20d7n", "<s3>con", "<p>:|", "<p>\u02c6 n", "<s3>con", "<p> =| x", "<s3>obj", "<p> \u2212  x", "<s3>roller|", "", "<p>|| x", "<s3>obj", "<p> \u2212  x", "<s3>roller", "<p>||", "<s3>2|", "<p>(3)|", "<p>Using the fact that  \u03b4x", "<s3>cr", "<p> is orthogonal to  \u02c6 n", "<s3>con", "<p> and is also| in the plane formed by  \u03b4x", "<s3>cb", "<p> and  \u03b4x", "<s3>contact", "<p>, we can compute|", "<p>the direction of  \u03b4x", "<s3>cr", "<p> as:|", "<p>\ufffd| \u03b4x", "<s3>cr", "<p> =| ( \u03b4x", "<s3>cb", "<p> \u00d7  \u03b4x", "<s3>contact", "<p>)  \u00d7  \u02c6 n", "<s3>con|", "", "<p>|| ( \u03b4x", "<s3>cb", "<p> \u00d7  \u03b4x", "<s3>contact", "<p>)  \u00d7  \u02c6 n", "<s3>con", "<p>||", "<s3>2|", "<p>(4)|", "<p>It is also easy to \ufb01nd that the direction of  \u03b4x", "<s3>cb", "<p> aligns with| Z", "<s3>2", "<p>:| \ufffd| \u03b4x", "<s3>cb", "<p> =  Z", "<s3>2|", "<p>(5)|", "<p>Projecting  \u03b4x", "<s3>contact", "<p> onto \ufffd| \u03b4x", "<s3>cb", "<p> and \ufffd| \u03b4x", "<s3>cr", "<p> gives  \u03b4x", "<s3>cb", "<p> and  \u03b4x", "<s3>cr", "<p>.| It is equivalent to solving  \u03b1  and  \u03b2  in equation:|", "<p>\u03b4x", "<s3>contact", "<p> =  \u03b1 \ufffd| \u03b4x", "<s3>cb", "<p> +  \u03b2 \ufffd| \u03b4x", "<s3>cr|", "<p>(6)|", "<p>By cross multiplying \ufffd| \u03b4x", "<s3>cb", "<p> and \ufffd| \u03b4x", "<s3>cr", "<p> to (6), respectively, we| can solve for  \u03b1  and  \u03b2 , and the resulting  \u03b4x", "<s3>cb", "<p> and  \u03b4x", "<s3>cr|", "<p>are shown below. Note that  \u02c6 n", "<s3>z", "<p> is only used to extract the| magnitude from the cross products.|", "<p>\u02c6 n", "<s3>z", "<p> =| \ufffd| \u03b4x", "<s3>cr", "<p> \u00d7 \ufffd| \u03b4x", "<s3>cb|", "", "<p>|| \ufffd| \u03b4x", "<s3>cr", "<p> \u00d7 \ufffd| \u03b4x", "<s3>cb", "<p>||", "<s3>2|", "<p>(7)|", "<p>\u03b4x", "<s3>cb", "<p> =  \u02c6 n", "<s3>z", "<p> \u00b7  ( \ufffd| \u03b4x", "<s3>cr", "<p> \u00d7  \u03b4x", "<s3>contact", "<p>)|", "<p>\u02c6 n", "<s3>z", "<p> \u00b7  ( \ufffd| \u03b4x", "<s3>cr", "<p> \u00d7 \ufffd| \u03b4x", "<s3>cb", "<p>)| \ufffd| \u03b4x", "<s3>cb|", "<p>(8)|", "<p>\u03b4x", "<s3>cr", "<p> =  \u02c6 n", "<s3>z", "<p>( \u00b7 \ufffd| \u03b4x", "<s3>cb", "<p> \u00d7  \u03b4x", "<s3>contact", "<p>)|", "<p>\u02c6 n", "<s3>z", "<p> \u00b7  ( \ufffd| \u03b4x", "<s3>cb", "<p> \u00d7 \ufffd| \u03b4x", "<s3>cr", "<p>)| \ufffd| \u03b4x", "<s3>cr|", "<p>(9)|", "<p>The joint velocity of base joint ( \u03c9", "<s3>1", "<p>) and roller joint ( \u03c9", "<s3>3", "<p>) can| be calculated using inverse kinematics.| The \ufb01nal step is to calculate the pivot angle \u02c6 \u03b8", "<s3>2", "<p> to align| the rolling direction with \ufffd| \u03b4x", "<s3>cr", "<p>:|", "<p>Z", "<s3>3", "<p> =  \u00b1| Z", "<s3>2", "<p> \u00d7 \ufffd| \u03b4x", "<s3>cr|", "", "<p>|| Z", "<s3>2", "<p> \u00d7 \ufffd| \u03b4x", "<s3>cr", "<p>||", "<s3>2|", "<p>(10)|", "<p>Note that because there is a joint limit at the pivot, we| are limiting  Z", "<s3>3", "<p> to always have a component along the  Z", "<s3>0|", "<p>direction. The pivot angle  \u03b8", "<s3>2", "<p> can be calculated by|", "<p>\u03b8", "<s3>2", "<p> = arccos| \ufffd| Z", "<s3>1", "<p> \u00b7  Z", "<s3>3|", "", "<p>|| Z", "<s3>1", "<p>||", "<s3>2", "<p>|| Z", "<s3>3", "<p>||", "<s3>2|", "", "<p>\ufffd| (11)|", "<p>The above process only describes how each joint should| move given the instantaneous desired velocity of the object;| no path planning is used for navigating the rollers given| the initial and target poses of the object. In our case, we| simply compute the difference between the initial and target| pose, and set the desired velocity equal to a scaling factor| multiplied by this computed difference:  \u03b4x", "<s3>obj", "<p> =  \u03bb \u2206 x", "<s3>obj", "<p>.| This method works very well for convex objects whose| radii of curvature do not change drastically. Experimental| validation of manipulating a cube is shown in Section V.|", "<p>B. Handcrafted Control Policy|", "<p>The handcrafted control policy is formulated according| to the results from the previous section. Given the current| object position and orientation, the target object position| and orientation, and the current joint values, the positions| of all nine joints are calculated for the following time step.| One difference between the implemented policy and the| theoretical policy is that the base joint velocities are not|", "<p>controlled based on the derivation above. Instead, they are| position controlled to a setpoint in order for the rollers to| maintain contact with the object. This is because we are| focusing on the rotation of the object instead of translation.| The main purpose of the base joint in our case is to keep the| rollers in contact with the object. The analytical approach| presented in the previous section performs both translation| and rotation. The translation capability of the grasper is| demonstrated in the complementary video through scripted| movements. However, the translation capability is relatively| limited compared to the rotation capability, which is why we| decided to focus on object rotation in the control policy.|", "<p>C. Imitation Learning Algorithm|", "<p>We adopted imitation learning, speci\ufb01cally  Behavior| Cloning  in order to learn how to transform an object. The| optimal policy was learned through expert demonstrations| characterized by the above handcrafted control policy.  Be-| havior Cloning  is particularly useful when it is dif\ufb01cult to| explicitly specify the reward function.| In our case, the state space is de\ufb01ned as  |S| \u2208  R", "<s3>35", "<p>, and| consists of the following: the current state of the grasper| ( s", "<s3>1", "<p> \u2192  s", "<s3>9", "<p>), current object position ( s", "<s3>10", "<p> \u2192  s", "<s3>12", "<p>), current| object quaternion ( s", "<s3>13", "<p> \u2192  s", "<s3>16", "<p>), previous object position| ( s", "<s3>17", "<p> \u2192  s", "<s3>19", "<p>), previous object quaternion ( s", "<s3>20", "<p> \u2192  s", "<s3>23", "<p>),| object termination position ( s", "<s3>24", "<p> \u2192  s", "<s3>26", "<p>), object termination| orientation in angle-axis representation ( s", "<s3>27", "<p> \u2192  s", "<s3>29", "<p>), object| initial position ( s", "<s3>30", "<p> \u2192  s", "<s3>32", "<p>), and object initial orientation in| angle-axis representation ( s", "<s3>33", "<p> \u2192  s", "<s3>35", "<p>). The action space is| de\ufb01ned as  |A| \u2208  R", "<s3>9", "<p>and contains the nine joint positions| ( a", "<s3>1", "<p> \u2192  a", "<s3>9", "<p>).| We constructed a deep neural network to determine the| actions for each of the nine gripper joints. The network| consisted of three fully connected hidden layers with leaky| ReLU activations, except at the output layer, and 256 nodes| in each hidden layer.| The handcrafted control policy from the previous section| was used to generate  N  expert trajectories, which are simply| a series of state-action pairs. The  i th trajectory is de\ufb01ned as:|", "<p>T", "<s3>( i )", "<p>= [( s", "<s3>( i )| 0", "<p>, a", "<s3>( i )| 0", "<p>) ,  ( s", "<s3>( i )| 1", "<p>, a", "<s3>( i )| 1", "<p>) , . . .  ]| (12)|", "<p>We \ufb01rst trained our policy  \u03c0", "<s3>0", "<p>( s", "<s3>i", "<p>)  to predict the expert| action by minimizing the loss  L  between  \u03c0", "<s3>0", "<p>( s", "<s3>i| j", "<p>)  and  a", "<s3>i| j", "<p>in| a supervised approach:|", "<p>L", "<s3>i| j", "<p>=  \u2225 \u03c0", "<s3>0", "<p>( s", "<s3>i| j", "<p>)  \u2212  a", "<s3>i| j", "<p>\u2225", "<s3>2", "<p>,  \u2200  s", "<s3>j", "<p>\u2208 |T", "<s3> i", "<p>| , i  \u2208  N| (13)|", "<p>Subsequent policy updates are computed according to DAg-| ger [28].| We also implemented a method to increase the number| of expert demonstrations iteratively. For a given object,| every object transformation trajectory is speci\ufb01ed by a 12-| dimensional vector containing the object starting and tar-| get position and orientation:  ( x", "<s3>i| obj,s", "<p>, q", "<s3>i| obj,s", "<p>, x", "<s3>i| obj,t", "<p>, q", "<s3>i| obj,t", "<p>) . By| imitating the expert demonstration examples, we learn a| policy supported by these expert demonstrations. This control| embedding is able to interpolate between known trajectories|", "<p>using a nearest neighbor policy in order to generate trajec-| tories for previously unseen transformations.| Based on the learned policy, we accumulate nearby trans-| formations. Let  D  = ( x", "<s3>i| obj,s", "<p>, q", "<s3>i| obj,s", "<p>, x", "<s3>i| obj,t", "<p>, q", "<s3>i| obj,t", "<p>)  represent| the transformations which our policy already knows. If an| interpolated trajectory transformation is nearby to one of the| transformations in  D , we add the transformation to  D , and| then continue to train our policy based on the transformation| demonstrations in  D . Through this fashion, we kept growing| and maintaining a traversable graph of transforming the| object between various poses, similar to [35]. The learned| control policy could also be treated as an ef\ufb01cient initializa-| tion for a deep reinforcement learning approach or motion| planning for in-hand manipulations.|", "<p>V. E", "<s2>XPERIMENTS|", "", "<p>A. Simulation Experiments|", "<p>Mujoco 2.0 was used to simulate both the handcrafted| control policy and the learned policy before transferring| them to the physical setup (see Fig. 6). By default, Mujoco| assumes a soft-contact model which leads to relatively slip-| pery contacts, but the solver parameters ( solref  and  solimp )| were changed to have a harder contact model in order to| better model the friction between the object and the spherical| rollers. An elliptical friction cone was used along with the| Newton solver to evaluate these constraints.| The gripper was modeled in the software, and the fol-| lowing setup was speci\ufb01ed. The base joints were position| controlled, and had their force output clamped; these settings| allowed the \ufb01ngers to act compliantly and conform to an| object\u2019s shape as it was reoriented, and stabilized the grasp.| The pivots had a rotation range from  [ \u2212", "<s3>\u03c0|", "<s3>2", "<p>,", "<s3> \u03c0|", "<s3>2", "<p>]  (with the zero| position corresponding with the roller axis aligned along the| length of the \ufb01nger) in order to represent the limited range| in the physical setup due to motor wires. A pivot rotation| threshold of  3", "<s3>\u25e6", "<p>per time step was used to prohibit quick| jumps between the two rotation limits in favor of smoother| motion that stabilized the simulation.| Sensors were placed at all nine joints to read joint posi-| tions and two sensors were placed on the object to obtain| its orientation (quaternion) and 3D position relative to the| global frame. The learned policy outputted values for all nine| joints. The handcrafted control policy only used the latter two|", "", "<s2>Fig. 6.| A visualization of our simulation in Mujoco. In simulation the| gripper maintains the same physical design as the real gripper.|", "<s2>Fig. 7.| Top row: the cube being manipulated from a starting position| shown on the left. Bottom row: the novel object, a rectangular prism being| similarly manipulated|", "", "<p>sensors and the base joint positions to calculate the output| velocities for the pivots and rollers at every time step.| Experiments were run by specifying a desired orientation| in angle-axis representation| \ufffd| [ x, y, z ]", "<s3>T", "<p>, \u03b8| \ufffd| . Most experi-| ments were carried out with a 6 cm cube with a mass of| 32.4 grams that had a starting quaternion of  q", "<s3>0", "<p> = [1 ,  0 ,  0 ,  0] .| Object rotation angles were speci\ufb01ed as at least  90", "<s3>\u25e6", "<p>in the| majority of experiments to ensure that the rollers had to| traverse across the edges of the cube. Rotation axes ranged| from more vertical axes \u2013 which were generally easier for| the simulation to realize \u2013 to purely horizontal axes which| were more dif\ufb01cult.|", "<p>B. Experimental Setup|", "<p>The experimental setup included the grasper, an overhead| Intel Realsense D400series RGBD camera, and various 3D| printed objects including a cube, a cube with \ufb01lleted edges,| and spheres of various masses and sizes. The handcrafted| control policy was able to be run both open-loop and closed-| loop. Since object orientation and position were used as input| to the control policy, only the cube was run in the closed-loop| operation with QR-tags on all 6 faces. Open-loop runs with| the spheres were used to qualitatively verify the handcrafted| control policy on the hardware.| At the start of each experiment the \ufb01ngers would spread| out, allowing a human operator to hold the cube at approx-| imately the initial position speci\ufb01ed for that particular trial.| The \ufb01ngers would then move inwards and grasp the object.| At every time step the joint positions, object orientation, and| position would be read-in, from which the corresponding| joint output would be calculated and sent to the actuators.|", "<p>C. Evaluation Metric|", "<p>We adopted the orientation error metric suggested in [36]| which can be computed by using the sum of quaternions and| normalizing. Given the desired object orientation,  q", "<s3>obj,d", "<p>, and| the current object orientation,  q", "<s3>obj,c", "<p>, the error is calculated| as the following:|", "", "<s2>Fig. 8. Orientation error for real world experiment results. The vertical axes| represents the orientation error de\ufb01ned in V-C. The horizontal lines repre-| sents various experiments.  S D  and  N  denote simple manipulations tasks,| dif\ufb01cult manipulation tasks and novel object transformations,respectively.| Notes: (1) The handcrafted control is designed to stop at a pre-de\ufb01ned| error threshold, therefore its orientation error is not meaningful to shown.| (2) Imitation learning policy in simulation is deterministic. (3) The novel| object transformation was only tested in real-world experiments, thus no| simulation results were available.|", "", "<p>e", "<s3>\u03c9", "<p> = 100 min( || q", "<s3>obj,d", "<p> \u2212  q", "<s3>obj,c", "<p>||", "<s3>2", "<p>,  || q", "<s3>obj,d", "<p> +  q", "<s3>obj,c", "<p>||", "<s3>2", "<p>)| \u221a|", "<p>2| (14)|", "<p>VI. R", "<s2>ESULTS AND", "<p> A", "<s2>NALYSIS|", "", "<p>The results of the experiments carried out on the hard-| ware are presented in Fig. 8. All reported percent average| errors were calculated according to (14). The experiments| were divided up as follows: simple transformations, which| had axes of rotations with strong  Z", "<s3>G", "<p> components, dif\ufb01cult| transformations, which had axes of rotations with strong  x| and  y  components, and novel object transformations, which| consisted of transforming an elongated rectangular prism| with \ufb01lleted edges (6 cm  \u00d7  6 cm  \u00d7  8 cm) (Fig. 7). The| reason transformations with strong  Z", "<s3>G", "<p> components for the| rotation axis were easier was due to the three-\ufb01nger design| of the grasper, leading to a much better force closure around| X", "<s3>G", "<p> \u2212  Y", "<s3>G", "<p> direction than the  Z", "<s3>G", "<p> direction. All experiments| speci\ufb01ed the target as an axis and a  90", "<s3>\u25e6", "<p>rotation about| that axis. This choice of a rotation angle is described in| Simulation Experiments .|", "<p>A. Simple Cases & Sim-to-Real Transferring|", "<p>The imitation learning method performed better for 3 out| of the 5 simple transformation cases ( S", "<s3>1", "<p> \u2192  S", "<s3>5", "<p>). The average| orientation error across these 5 trials shows that the imitation| learning method ( 15 . 3% \u00b1 2 . 2% ) and the handcrafted control| policy ( 13 . 6%  \u00b1  7 . 2% ) are comparable when performing| object rotations about more vertical axes. However, the| handcrafted control policy has more than 3 times the standard| deviation of the imitation learning.| As with any physical system there is a performance dif-| ference when compared to the simulation setup. Fortunately,| this difference was small with the imitation learning\u2019s results| in simulation reporting an average error of  15 . 4% . We believe| this is due to inclusion of sensor noise in the simulation, as|", "<p>well as \ufb01ne tuning of the contact model to more closely align| with observations of the physical system performance.|", "<p>B. Untrained Target Pose Test Cases|", "<p>For the dif\ufb01cult transformations ( D", "<s3>1", "<p> \u2192  D", "<s3>6", "<p>), the imitation| learning outperformed the handcrafted policy across all 6| trials by an average percent average error of  16 . 0% . Three of| these trials,  D", "<s3>1", "<p> \u2192  D", "<s3>3", "<p>, including target poses not trained in| simulation. Overall, the imitation learning achieved a percent| average error of  25 . 3% \u00b1 4 . 2% , while the handcrafted control| policy achieved  41 . 3% \u00b1 5 . 8% . Again, the imitation learning| was able to demonstrate more stable trajectories despite the| sensor noise.|", "<p>C. Novel Object Test Cases|", "<p>Tests with the novel object ( N", "<s3>1", "<p> \u2192  N", "<s3>2", "<p>), a rectangular| prism, demonstrated similar results to the simple transforma-| tion case. The percent average errors were comparable be-| tween imitation learning ( 17 . 1% \u00b1 3 . 9% ) and the handcrafted| control policy ( 20 . 3%  \u00b1  10 . 3% ). However, the handcrafted| control policy had a much higher standard deviation.|", "<p>D. Discussion|", "<p>A potential explanation for the poorer performance of the| handcrafted control policy, especially for the dif\ufb01cult trans-| formations, was that it always generated linear trajectories| from the current position to the target. However, in many| cases, especially for rotation axes with dominating horizontal| components, taking a linear path can lead to the gripper| dropping the object. As the object is rotated closer to the| desired axis of rotation, one or more of the rollers can| roll onto a surface where the contact normal has a  \u2212 Z", "<s3>G|", "<p>component; the contact normal can overcome the frictional| support provided by another roller and result in dropping| the object. This problem can be mitigated by increasing| the positional error compensation in the closed-loop control| method in order to more actively control the object height.| Unfortunately, placing a greater emphasis on controlling| the object position was seen to cause previously successful| trajectories to fail. No combination of gains was seen to| work across all observed successes. On the other hand,| the imitation learning was better for a couple of potential| reasons: (1) the policy was learned from noisy sensor data| which increased its robustness, and (2) the discovery of safe| and repeatable trajectories.|", "<p>VII. C", "<s2>ONCLUSION AND", "<p> F", "<s2>UTURE", "<p> W", "<s2>ORK|", "", "<p>This paper presents Roller Grasper V2, a new design for| a grasper based on steerable spherical rollers located at the| \ufb01nger tips. The hardware design choices and engineering| details were provided. A handcrafted control policy was| constructed that utilized the active surfaces of the rollers to| transform an object to an arbitrary target pose. This control| policy was used to generate expert trajectories in order to| develop an imitation learning based policy. This learned| policy was able to interpolate between learned trajectories in| order to successfully reach new targets; these new trajectories|", "<p>were placed in a buffer of known trajectories in order to| develop a traversable graph. Experiments for the handcrafted| policy and the learned policy demonstrated the utility of this| graph for generating safe trajectories, as the learned policy| outperformed the handcrafted policy for all dif\ufb01cult target| poses. For simple cases, the two policies performed compa-| rably, except the learned policy had much lower variance| in the trajectory. Future work includes reducing the size| of the rollers to allow for manipulation of smaller objects,| developing a novel mechanism that solves the non-holonomic| constraints and incorporating tactile sensing on the rollers to| provide high-\ufb01delity feedback.| R", "<s2>EFERENCES|", "<s2>[1] S. Yuan, A. D. Epps, J. B. Nowak, and J. K. Salisbury, \u201cDesign of| a roller-based dexterous hand for object grasping and within-hand| manipulation,\u201d in  2020 IEEE International Conference on Robotics| and Automation , May 2020.|", "<s2>[2] C. Piazza, G. Grioli, M. Catalano, and A. Bicchi, \u201cA century of robotic| hands,\u201d  Annual Review of Control, Robotics, and Autonomous Systems ,| vol. 2, pp. 1\u201332, 2019.|", "<s2>[3] M. A. Diftler, J. S. Mehling, M. E. Abdallah, N. A. Radford, L. B.| Bridgwater, A. M. Sanders, R. S. Askew, D. M. Linn, J. D. Yamokoski,| F. A. Permenter, B. K. Hargrave, R. Platt, R. T. Savely, and R. O.| Ambrose, \u201cRobonaut 2 - the \ufb01rst humanoid robot in space,\u201d in| 2011 IEEE International Conference on Robotics and Automation ,| pp. 2178\u20132183, May 2011.|", "<s2>[4] Shadow Robot Company, \u201cDesign of a dextrous hand for advanced| CLAWAR applications,\u201d in  Proceedings of the 6th International con-| ference on climbing and walking robots and the supporting technolo-| gies for mobile machines , pp. 691\u2013698, September 2003.|", "<s2>[5] M. Grebenstein, A. Albu-Sch\u00a8affer, T. Bahls, M. Chalon, O. Eiberger,| W. Friedl, R. Gruber, S. Haddadin, U. Hagn, R. Haslinger, H. H\u00a8oppner,| S. J\u00a8org, M. Nickl, A. Nothhelfer, F. Petit, J. Reill, N. Seitz,| T. Wimb\u00a8ock, S. Wolf, T. W\u00a8usthoff, and G. Hirzinger, \u201cThe dlr hand| arm system,\u201d in  2011 IEEE International Conference on Robotics and| Automation , pp. 3175\u20133182, May 2011.|", "<s2>[6] R. R. Ma and A. M. Dollar, \u201cAn underactuated hand for ef\ufb01cient| \ufb01nger-gaiting-based dexterous manipulation,\u201d in  2014 IEEE Inter-| national Conference on Robotics and Biomimetics (ROBIO 2014) ,| pp. 2214\u20132219, Dec 2014.|", "<s2>[7] A. M. Dollar and R. D. Howe, \u201cThe highly adaptive sdm hand: Design| and performance evaluation,\u201d  The International Journal of Robotics| Research , vol. 29, no. 5, pp. 585\u2013597, 2010.|", "<s2>[8] H. Stuart, S. Wang, O. Khatib, and M. R. Cutkosky, \u201cThe ocean| one hands: An adaptive design for robust marine manipulation,\u201d  The| International Journal of Robotics Research , vol. 36, no. 2, pp. 150\u2013| 166, 2017.|", "<s2>[9] N. Rojas, R. R. Ma, and A. M. Dollar, \u201cThe gr2 gripper: an un-| deractuated hand for open-loop in-hand planar manipulation,\u201d  IEEE| Transactions on Robotics , vol. 32, no. 3, pp. 763\u2013770, 2016.|", "<s2>[10] N. Govindan and A. Thondiyath, \u201cDesign and Analysis of a Multi-| modal Grasper Having Shape Conformity and Within-Hand Manipu-| lation With Adjustable Contact Forces,\u201d  Journal of Mechanisms and| Robotics , vol. 11, 07 2019. 051012.|", "<s2>[11] V. Tincani, M. G. Catalano, E. Farnioli, M. Garabini, G. Grioli,| G. Fantoni, and A. Bicchi, \u201cVelvet \ufb01ngers: A dexterous gripper| with active surfaces,\u201d in  2012 IEEE/RSJ International Conference on| Intelligent Robots and Systems , pp. 1257\u20131263, Oct 2012.|", "<s2>[12] R. R. Ma and A. M. Dollar, \u201cIn-hand manipulation primitives for| a minimal, underactuated gripper with active surfaces,\u201d in  proceed-| ings of the ASME 2016 International Design Engineering Technical| Conferences (IDETC), Computers and Information in Engineering| Conference (CIE) , p. V05AT07A072, 2016.|", "<s2>[13] V. Kumar, E. Todorov, and S. Levine, \u201cOptimal control with learned lo-| cal models: Application to dexterous manipulation,\u201d in  2016 IEEE In-| ternational Conference on Robotics and Automation (ICRA) , pp. 378\u2013| 383, IEEE, 2016.|", "<s2>[14] O. M. Andrychowicz, B. Baker, M. Chociej, R. Jozefowicz, B. Mc-| Grew, J. Pachocki, A. Petron, M. Plappert, G. Powell, A. Ray,  et al. ,| \u201cLearning dexterous in-hand manipulation,\u201d  The International Journal| of Robotics Research , vol. 39, no. 1, pp. 3\u201320, 2020.|", "<s2>[15] N. Chavan-Da\ufb02e, R. Holladay, and A. Rodriguez, \u201cIn-hand manipula-| tion via motion cones,\u201d  arXiv preprint arXiv:1810.00219 , 2018.|", "<s2>[16] S. Cruciani, C. Smith, D. Kragic, and K. Hang, \u201cDexterous manipula-| tion graphs,\u201d in  2018 IEEE/RSJ International Conference on Intelligent| Robots and Systems (IROS) , pp. 2040\u20132047, IEEE, 2018.|", "<s2>[17] M. Liarokapis and A. M. Dollar, \u201cDeriving dexterous, in-hand ma-| nipulation primitives for adaptive robot hands,\u201d in  2017 IEEE/RSJ| International Conference on Intelligent Robots and Systems (IROS) ,| pp. 1951\u20131958, IEEE, 2017.|", "<s2>[18] N. Rahman, L. Carbonari, M. D\u2019Imperio, C. Canali, D. G. Caldwell,| and F. Cannella, \u201cA dexterous gripper for in-hand manipulation,\u201d in| 2016 IEEE International Conference on Advanced Intelligent Mecha-| tronics (AIM) , pp. 377\u2013382, July 2016.|", "<s2>[19] W. G. Bircher, A. M. Dollar, and N. Rojas, \u201cA two-\ufb01ngered robot grip-| per with large object reorientation range,\u201d in  2017 IEEE International| Conference on Robotics and Automation (ICRA) , pp. 3453\u20133460, May| 2017.|", "<s2>[20] Y. Karayiannidis, K. Pauwels, C. Smith, D. Kragic,  et al. , \u201cIn-hand| manipulation using gravity and controlled slip,\u201d in  2015 IEEE/RSJ| International Conference on Intelligent Robots and Systems (IROS) ,| pp. 5636\u20135641, IEEE, 2015.|", "<s2>[21] D. L. Brock, \u201cEnhancing the dexterity of a robot hand using controlled| slip,\u201d in  Proceedings. 1988 IEEE International Conference on Robotics| and Automation , pp. 249\u2013251, IEEE, 1988.|", "<s2>[22] N. C. Da\ufb02e, A. Rodriguez, R. Paolini, B. Tang, S. S. Srinivasa,| M. Erdmann, M. T. Mason, I. Lundberg, H. Staab, and T. Fuhlbrigge,| \u201cExtrinsic dexterity: In-hand manipulation with external forces,\u201d in| 2014 IEEE International Conference on Robotics and Automation| (ICRA) , pp. 1578\u20131585, IEEE, 2014.|", "<s2>[23] J. Shi, J. Z. Woodruff, P. B. Umbanhowar, and K. M. Lynch, \u201cDynamic| in-hand sliding manipulation,\u201d  IEEE Transactions on Robotics , vol. 33,| no. 4, pp. 778\u2013795, 2017.|", "<s2>[24] N. Chavan-Da\ufb02e and A. Rodriguez, \u201cPrehensile pushing: In-hand| manipulation with push-primitives,\u201d in  2015 IEEE/RSJ International| Conference on Intelligent Robots and Systems (IROS) , pp. 6215\u20136222,| IEEE, 2015.|", "<s2>[25] C. Eppner, R. Deimel, J. Alvarez-Ruiz, M. Maertens, and O. Brock,| \u201cExploitation of environmental constraints in human and robotic| grasping,\u201d  The International Journal of Robotics Research , vol. 34,| no. 7, pp. 1021\u20131038, 2015.|", "<s2>[26] H. Van Hoof, T. Hermans, G. Neumann, and J. Peters, \u201cLearning robot| in-hand manipulation with tactile features,\u201d in  2015 IEEE-RAS 15th| International Conference on Humanoid Robots (Humanoids) , pp. 121\u2013| 127, IEEE, 2015.|", "<s2>[27] D. A. Pomerleau, \u201cAlvinn: An autonomous land vehicle in a neural| network,\u201d in  Advances in neural information processing systems ,| pp. 305\u2013313, 1989.|", "<s2>[28] S. Ross, G. Gordon, and D. Bagnell, \u201cA reduction of imitation learning| and structured prediction to no-regret online learning,\u201d in  Proceedings| of the fourteenth international conference on arti\ufb01cial intelligence and| statistics , pp. 627\u2013635, 2011.|", "<s2>[29] A. Y. Ng, S. J. Russell,  et al. , \u201cAlgorithms for inverse reinforcement| learning.,\u201d|", "<s2>[30] L. Shao, T. Migimatsu, Q. Zhang, K. Yang, and J. Bohg, \u201cCon-| cept2robot: Learning manipulation concepts from instructions and| human demonstrations,\u201d in  Proceedings of Robotics: Science and| Systems (RSS) , July 2020.|", "<s2>[31] A. Billard, Y. Epars, S. Calinon, S. Schaal, and G. Cheng, \u201cDiscov-| ering optimal imitation strategies,\u201d  Robotics and autonomous systems ,| vol. 47, no. 2-3, pp. 69\u201377, 2004.|", "<s2>[32] P. Pastor, H. Hoffmann, T. Asfour, and S. Schaal, \u201cLearning and| generalization of motor skills by learning from demonstration,\u201d in  2009| IEEE International Conference on Robotics and Automation , pp. 763\u2013| 768, IEEE, 2009.|", "<s2>[33] M. R. Cutkosky and I. Kao, \u201cComputing and controlling compliance| of a robotic hand,\u201d  IEEE transactions on robotics and automation ,| vol. 5, no. 2, pp. 151\u2013165, 1989.|", "<s2>[34] M. T. Mason and J. K. Salisbury Jr, \u201cRobot hands and the mechanics| of manipulation,\u201d 1985.|", "<s2>[35] S. Cruciani, K. Hang, C. Smith, and D. Kragic, \u201cDual-arm in-hand| manipulation and regrasping using dexterous manipulation graphs,\u201d| arXiv preprint arXiv:1904.11382 , 2019.|", "<s2>[36] D. Q. Huynh, \u201cMetrics for 3d rotations: Comparison and analysis,\u201d| Journal of Mathematical Imaging and Vision , vol. 35, no. 2, pp. 155\u2013| 164, 2009.|"]