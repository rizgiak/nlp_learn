Design and Control of Roller Grasper V2 for In-Hand Manipulation

Shenli Yuan1,2, Lin Shao2, Connor L. Yako1,2, Alex Gruebele1, and J. Kenneth Salisbury2

0
2
0
2

v
o
N
7
1

]

O
R
.
s
c
[

2
v
9
9
4
8
0
.
4
0
0
2
:
v
i
X
r
a

Abstract The ability to perform in-hand manipulation
still remains an unsolved problem; having this capability
would allow robots to perform sophisticated tasks requiring
repositioning and reorienting of grasped objects. In this work,
we present a novel non-anthropomorphic robot grasper with
the ability to manipulate objects by means of active surfaces at
the ngertips. Active surfaces are achieved by spherical rolling
ngertips with two degrees of freedom (DoF)  a pivoting
motion for surface reorientation  and a continuous rolling
motion for moving the object. A further DoF is in the base
of each nger, allowing the ngers to grasp objects over a
range of size and shapes. Instantaneous kinematics was derived
and objects were successfully manipulated both with a custom
handcrafted control scheme as well as one learned through
imitation learning,
in simulation and experimentally on the
hardware.

I. INTRODUCTION

In an effort to bring robots from the laboratory into real
world environments, researchers have endeavored to develop
increasingly dexterous robots that can interact deftly with
objects. In order for such robots to take on a wide range
of everyday tasks, they need to be capable of sophisticated
object manipulation. Many vital higher-level tasks will rely
on a robotÅfs capability to perform in-hand manipulation by
reorienting objects while maintaining the grasp. Out of all
the grasping and manipulation tasks, in-hand manipulation
is among the ones that require the most dexterity.

A background of in-hand manipulation literature is pre-
sented in [1] and a more extensive review of robot hands and
graspers is given in [2]. Common approaches to designing
robotic grippers that can perform in-hand manipulation are:
anthropomorphic hands which take advantage of intrinsic
human dexterity, but due to the high number of degrees
of freedom are complex and expensive [3] [4] [5]; under-
actuated hands which passively conform to objects, achieving
good grasp stability, but at the cost of the controllability
needed to perform many in-hand manipulation tasks [6] [7]
[8] [9]; grippers with active surfaces (such as conveyors)
which allow for the object to be manipulated without chang-
ing grasp pose, but with a xed conveyor orientation limiting
possible motions [10] [11] [12].

We developed a novel grasper design using articulated, ac-
tively driven spherical rollers located at the ngertips, shown
in Fig. 1. By incorporating continuous rotating mechanisms,

*This work has been funded, in part, by the Stanford Interdisciplinary
Graduate Fellowship. Detailed implementations can be found at: https://
ccrma.stanford.edu/shenliy/roller_grasper_v2.html

1Department of Mechanical Engineering, Stanford University
2Stanford Articial Intelligence Lab (SAIL), Stanford University
{shenliy, lins2, clyako, agruebe2}@stanford.edu,

jks@cs.stanford.edu

Fig. 1. The Roller Grasper V2 prototype mounted on a UR-5 robot arm.
Three ngers, each with three degrees of freedom, can grasp and reorient
an object using rolling contact with the rubber coated rollers

it is possible to create graspers that are highly capable but
relatively simple by design. The active surface achieved by
rolling and reorientation of the spherical rollers allow the
grasper to perform in-hand manipulation without the need
for nger gaiting. Rolling contact on the object can be
viewed as a motion that continuously breaks contact with
the object while simultaneously re-establishing contact at
adjacent locations. The ability to reorient an object to any
pose also lessens the need to use externally actuated degrees
of freedom (e.g. actuation of the robotic arm and wrist)
which simplies the control scheme. More importantly, the
spherical design of the nger tips allows for stable grasps
independent from the roller orientations, eliminating the need
to analyze grasping modes for different combinations of
roller orientations.

Learning robust policies for in-hand manipulation has been
a long-standing challenge in robotics due to the complex-
ity of modelling the object and grasper contacts and the
difculty of controlling nger motion in long and compli-
cated manipulation sequences. Deep Reinforcement Learning
(DRL) has been used to learn dexterous manipulation [13]
[14]. However, learning to hold the object rmly and stably
while transforming the object with deep reinforcement learn-
ing requires many more training episodes and a carefully
designed reward function. To overcome these issues, we used
an imitation learning based approach to learn a control policy
in order to arbitrarily transform an object while holding it.

 
 
 
 
 
 
We demonstrated the effectiveness of this learned policy both
in simulation and in real world experiments.

Our in-hand manipulation system consisted of a 3-ngered
grasper with spherical rollers at the ngertips, an overhead
RGBD camera, and objects with QR-tags on all faces. A
handcrafted control policy and an imitation learning policy
were developed to perform complex in-hand object trans-
formations. To the best of our knowledge, this work is the
rst attempt at developing a grasper with active surfaces
at the ngertips that transforms grasped objects through an
imitation learning policy. The paper is structured as follows:
we rst discuss our previous iteration of this robotic grasper
as well as other design and algorithmic approaches to robotic
grasping/in-hand manipulation (Section II). Section III then
briey describes the hardware. Section IV discusses the
formulation of the handcrafted control policy as well as
the imitation learning approach. The paper then provides an
overview of the experimental setup in simulation and in real
life, and concludes by reporting and discussing the results
(Section V and Section VI).

II. RELATED WORK

A. Previous Grasper Design

Our previous work [1] used articulated, actively driven
cylindrical rollers at the ngertips of a grasper to explore
imparting motion within a grasp using active surfaces. The
grasper used three modular 3-DoF ngers, and demonstrated
full 6-DoF spatial manipulation of objects including a sphere,
cube, and cylinder, as well as various grasping modalities.
One limitation of the previous roller grasper is the grasp
stability. Due to the cylindrical design of the nger tips,
several grasping congurations are unstable, resulting in
undetermined manipulation behaviors. The redundant com-
binations of grasping congurations also complicates the
control scheme as the conguration used is dependent on
specic manipulation tasks and the object being manipulated.

B. In-Hand Manipulation

In-hand manipulation is an age-old question in robotics
with a rich literature, from two-ngered grippers [15] [16],
to dexterous hands [13]. We briey review the relevant in-
hand manipulation works in this subsection. To achieve in-
hand manipulation, multi-ngered dexterous hands utilize the
redundancy of the ngers to move the object while holding
it. Under-actuated hands are able to leverage model based
control [17] for in-hand manipulation tasks. There are also
task-specic designs of under-actuated hands which enable
a limited set of repositioning skills [18] [19].

Other approaches to in-hand manipulation have been
explored which rely on gravity with controlled slip [20]
[21], induced accelerations [22] [23], or the environment
[15] [24] [25] to reduce the dexterity required of the hand.
However, such approaches require complex control and mod-
eling schemes or dependency on available environmental
geometry.

Contrary to modeling the complex dynamics involved in
grasping and object reorientation, some researchers have

opted to use reinforcement
learning (RL) to search for
optimal policies. This is especially useful when using un-
deractuated graspers or graspers with high DoFÅfs. In [26] an
underactuated grasper with tactile sensors on the ngertips
was used to horizontally slide a wooden cylinder back and
forth by rolling it along each nger. The learned policy was
evaluated on cylinders of different masses, sizes, and friction
coefcients. They found that the policy performed better than
a hard-coded control policy, which was used as a baseline,
but still struggled with cylinders with low-friction properties.
DRL has also been implemented successfully on the 24 DoF
Shadow Hand for dynamically moving a cylinder in hand and
for arbitrarily reorientating a cube using visual information
[13] [14]. However, our grasper needs to maintain hold of
the object solely through friction at the roller contact during
the manipulation process. This means that a tiny perturbation
of the ngertip could possibly break the contact and lead to
a dropped object. Therefore, the space of successful policies
is incredibly small relative to the entire policy space. Since
exploration is necessary in any DRL problem, it is difcult
for the algorithm to converge to the optimal policy. To
avoid this problem, we instead adopted an imitation learning
method, which will be discussed in the next section.

C. Imitation Learning Methods

Imitation learning aims to learn control policies by ob-
serving expert demonstrations. There are in general
two
types of approaches to tackle an imitation learning problem.
Behaviour cloning aims to train the agent to learn a mapping
from observations to actions given demonstrations,
in a
supervised learning fashion [27] [28]. Another approach is
Inverse Reinforcement Learning [29] [?], which attempts to
learn a reward function that describes the given demonstra-
tions. Our method falls under the scope of Behavior Cloning
which has led to many successes in robotics [31] [32]; our
approach is based on one Behaviour Cloning method called
DAgger [28]. To tackle the problem of generating expert
demonstrations, we also develop a method to accumulate
the demonstration examples iteratively starting from a few
expert demonstrations.

A. Hardware Design

III. DESIGN

The gripper (Fig. 2(a)) consists of three ngers, each
having three degrees of freedom (DoF). The rst DoF is
at the base of each nger and consists of a revolute joint
directly driven by a Robotis Dynamixel XM430-W350 smart
actuator. The other two DoF are located at each ngertip,
and are responsible for steering and rolling. The second
joint shown in Fig. 2(a) is orthogonal to the rst DoF, and
is driven by a micro DC motor with built-in gearbox and
quadrature encoder (Servocity No.638099). For a compact
form-factor, this actuator is located remotely from the axis
of rotation through a timing belt (Fig. 2(b)), and allows the
roller assembly to be pitched up to 180 degrees. The nal
DoF is actuated using the same type of geared motor but
housed inside the roller assembly (Fig. 2(c)), allowing it to

Fig. 2. A CAD model of the grasper: (a) the three degrees of freedom each nger has (b) an exploded view of each nger (c) an exploded view of the
roller assembly that contacts the object being manipulated

TABLE I
PHYSICAL PROPERTIES

Property
Link a (referred in Fig. 4)
Link b (referred in Fig. 4)
Link r (referred in Fig. 4)
Whole grasper weight
Maximum normal force at the ngertip
Maximum roller shear force

Value
48mm
122mm
21.5mm
700g
33.6N
16.4N

perform continuous rotation of the spherical contact surface
without its cables winding. The roller is encased in a pair of
2mm-thick semi-spherical silicone covers to provide a high-
friction surface for grasping and manipulation. The silicone
covers are molded from Mold Star 16 Fast (SmoothOn),
cast in a 2-part 3D printed mold (Objet Vero white, glossy
surface nish), with a coat of Ease Release 2000 release
agent. They are adhered to the 3D printed roller surfaces
using silicone compatible tape (McMaster No.7213A31) The
reference frame of the grasper is shown in Fig. 4 and the key
physical parameters are listed in Table I.

The design allows for unbounded reorientation of the
grasped object, while the range of translation is determined
by various factors such as the physical dimensions, shape,
in-hand orientation, mass, and resulting friction coefcient
of the grasped objects.

B. System Architecture

The system architecture used to operate the gripper is
shown in Fig. 3. A high-level API was developed to inter-
face between the low-level (hardware) information and the
manipulation algorithm. Information transferred during the
bidirectional communication includes positions for each joint
of the ngers, the current limit of the base joint, as well as
the control parameters for controlling the motors. Current to
the Dynamixel motors is used to set the stiffness of the base
joints and estimate the force exerted by the object on each
nger during manipulation.

A Teensy 3.6 microcontroller is used to handle communi-
cation with the high-level API as well as low-level control

Fig. 3. System architecture

of the motors. The six micro gearmotors located at
the
intermediate joints and within the rollers are controlled by
PD position controllers handled by the microcontroller. The
Dynamixel motors each run local PD control and commu-
nicate with the Teensy microcontroller through a TTL half-
duplex asynchronous serial communication protocol.

IV. TECHNICAL APPROACH

In this section we begin by describing the analytical
approach that led to development and implementation of a
handcrafted control policy. We then discuss how this control
policy was ne-tuned based on simulation results in order
to act as an expert trajectory generator for the imitation
learning. The nal subsection describes the imitation learning
formulation.

Initially, several DRL methods were used in place of imita-
tion learning. From these preliminary experiments we learned
that the space of successful policies, not necessarily optimal,
was especially small. This becomes a critical problem in
DRL given large policy spaces since signicant exploration
is necessary. Instead, we elected to pursue imitation learning
which proved to be both effective and efcient.

A. Analysis

Manipulating an object through rolling contact can be
viewed as navigating the rollers on the object. Therefore,
an object transformation can be achieved by navigating the
rollers from their initial contact locations to the desired nal
contact locations. While there is no unique solution for the
paths the rollers take during their navigation for given initial
and nal grasping poses, it is possible to solve, on this

É¬xcontact = JobjÉ¬xobj

(1)

where Jobj is the jacobian matrix mapping object motion to
the motion at the contact frame.

On the other hand, with the contact motion É¬xcontact, the
object position xobj, and roller position xroller all known,
the contact motion can be interpreted as the motion at the
contact location due to the movement of the 3 joints:

É¬xcontact = JÉ∆É¬É∆

(2)

Where JÉ∆ is the contact jacobian matrix for mapping nger
joint motions to motions in the contact coordinates, and
É¬É∆ is a vector of joint motions of the nger. In many
robotics applications É¬É∆ is determined directly by solving
(2). However, this method is not particularly applicable to
this gripper design for the following reasons:

1) The contact locations are very close to the pivot joint

2) The pivot joint has a joint limit of [Å| ÉŒ

so the nger is always close to a singularity.
2 , ÉŒ

2 ] meaning
that in many cases the instantaneous velocity required
from the pivot joint to achieve the desired contact
velocity cannot be reached.

Therefore, instead of carrying out the full inverse kine-
matics, we divide the contact motion into two components:
the component resulting from the motion of the base joint,
É¬xcb, and the component resulting from the rolling, É¬xcr.
The pivot joint is used to reorient the roller so that the
rolling direction at the contact point is aligned with É¬xcr.
This approximation is sufcient because (1) when the contact
locations are close to the pivot axis, the pivot motion does
not have a signicant impact on the object motion, and (2)
the object being grasped is over constrained by the three
ngers, so using the soft nger model [34] to approximate
roller contacts, the torque exerted by pivoting is compensated
by the lateral frictions from the two other ngers. Thus, the
singularity is advantageous in that it enables sole use of the
pivot joint to align the roller in order to achieve É¬xcr.

We developed a handcrafted control strategy based on
the above formulation with some modications that t the
gripper design. The handcrafted control policy tested in
simulation and on the hardware assumes that
the object
geometry is spherical with radius R. Specically, the below
calculations are for a single nger, but can be applied to
the two other ngers as well. For the simplicity of notation,
we use subscript 1, 2, and 3 to represent the base joint,
pivot joint, and roller joint, respectively, and leave out the
subscript A, B or C because the following derivation would
be identical for each nger.

In order to obtain É¬xcb and É¬xcr, we need to project
É¬xcontact onto Z2 and the contact plane (Fig. 5). The contact
plane can be described by its normal vector (cid:126)ncon:

ncon =

xobj Å| xroller
||xobj Å| xroller||2

(3)

Using the fact that É¬xcr is orthogonal to ncon and is also
in the plane formed by É¬xcb and É¬xcontact, we can compute

Fig. 4.
Reference frames of each of the three joints within each
nger, relative to the base coordinate system. Dashed lines indicate neutral
positions from which joint angles are measured.

non-holonomic system, for the instantaneous joint velocities
based on known object geometry, object pose, object veloc-
ity, and contact conditions (rolling without slipping).

Object geometry and pose are necessary to calculate the
contact locations, which determine the relationship between
joint motions and given contact motions on the rollers (the
contact jacobian matrix). This information can subsequently
be used to map the desired object motion to the motions
at the contact point on the object. Applying the contact
condition of rolling without slipping means that the contact
velocity on the object and the contact velocity on the roller
are equal. Thus, inverse kinematics can be used to calculate
the joint velocities required for a desired object motion, a
process similar to the calculation of geometric stiffness for
grasping presented in [33].

The problem is formulated as follows: given the objectÅfs
initial and target position and orientation, compute the con-
tact location and contact motion on the roller, and then
compute the pivot joint orientation and motions of the base
and roller joints.

Given the desired motion of the object and known contact

locations, we can obtain the contact motion by

Fig. 5. Contact motion breaks down to two components

the direction of É¬xcr as:

(cid:99)É¬xcr =

(É¬xcb Å~ É¬xcontact) Å~ ncon
||(É¬xcb Å~ É¬xcontact) Å~ ncon||2

(4)

It is also easy to nd that the direction of É¬xcb aligns with
Z2:

(cid:99)É¬xcb = Z2

(5)

Projecting É¬xcontact onto (cid:99)É¬xcb and (cid:99)É¬xcr gives É¬xcb and É¬xcr.
It is equivalent to solving Éø and É¿ in equation:

É¬xcontact = Éø(cid:99)É¬xcb + É¿(cid:99)É¬xcr

(6)

By cross multiplying (cid:99)É¬xcb and (cid:99)É¬xcr to (6), respectively, we
can solve for Éø and É¿, and the resulting É¬xcb and É¬xcr
are shown below. Note that nz is only used to extract the
magnitude from the cross products.

(7)

nz =

É¬xcb =

(cid:99)É¬xcr Å~ (cid:99)É¬xcb
||(cid:99)É¬xcr Å~ (cid:99)É¬xcb||2
nz  ((cid:99)É¬xcr Å~ É¬xcontact)
nz  ((cid:99)É¬xcr Å~ (cid:99)É¬xcb)
nz((cid:99)É¬xcb Å~ É¬xcontact)
nz  ((cid:99)É¬xcb Å~ (cid:99)É¬xcr)
The joint velocity of base joint (É÷1) and roller joint (É÷3) can
be calculated using inverse kinematics.

É¬xcr =

(cid:99)É¬xcr

(cid:99)É¬xcb

(9)

(8)

The nal step is to calculate the pivot angle É∆2 to align

the rolling direction with (cid:99)É¬xcr:

Z3 = Å}

Z2 Å~ (cid:99)É¬xcr
||Z2 Å~ (cid:99)É¬xcr||2

(10)

Note that because there is a joint limit at the pivot, we
are limiting Z3 to always have a component along the Z0
direction. The pivot angle É∆2 can be calculated by

É∆2 = arccos

(cid:18) Z1  Z3

(cid:19)

||Z1||2||Z3||2

(11)

The above process only describes how each joint should
move given the instantaneous desired velocity of the object;
no path planning is used for navigating the rollers given
the initial and target poses of the object. In our case, we
simply compute the difference between the initial and target
pose, and set the desired velocity equal to a scaling factor
multiplied by this computed difference: É¬xobj = É…xobj.
This method works very well for convex objects whose
radii of curvature do not change drastically. Experimental
validation of manipulating a cube is shown in Section V.

B. Handcrafted Control Policy

The handcrafted control policy is formulated according
to the results from the previous section. Given the current
object position and orientation, the target object position
and orientation, and the current joint values, the positions
of all nine joints are calculated for the following time step.
One difference between the implemented policy and the
theoretical policy is that the base joint velocities are not

controlled based on the derivation above. Instead, they are
position controlled to a setpoint in order for the rollers to
maintain contact with the object. This is because we are
focusing on the rotation of the object instead of translation.
The main purpose of the base joint in our case is to keep the
rollers in contact with the object. The analytical approach
presented in the previous section performs both translation
and rotation. The translation capability of the grasper is
demonstrated in the complementary video through scripted
movements. However, the translation capability is relatively
limited compared to the rotation capability, which is why we
decided to focus on object rotation in the control policy.

C. Imitation Learning Algorithm

We adopted imitation learning, specically Behavior
Cloning in order to learn how to transform an object. The
optimal policy was learned through expert demonstrations
characterized by the above handcrafted control policy. Be-
havior Cloning is particularly useful when it is difcult to
explicitly specify the reward function.

In our case, the state space is dened as |S| Å∏ R35, and
consists of the following: the current state of the grasper
(s1 Å® s9), current object position (s10 Å® s12), current
object quaternion (s13 Å® s16), previous object position
(s17 Å® s19), previous object quaternion (s20 Å® s23),
object termination position (s24 Å® s26), object termination
orientation in angle-axis representation (s27 Å® s29), object
initial position (s30 Å® s32), and object initial orientation in
angle-axis representation (s33 Å® s35). The action space is
dened as |A| Å∏ R9 and contains the nine joint positions
(a1 Å® a9).

We constructed a deep neural network to determine the
actions for each of the nine gripper joints. The network
consisted of three fully connected hidden layers with leaky
ReLU activations, except at the output layer, and 256 nodes
in each hidden layer.

The handcrafted control policy from the previous section
was used to generate N expert trajectories, which are simply
a series of state-action pairs. The ith trajectory is dened as:

0 , a(i)

T (i) = [(s(i)

0 ), (s(i)
We rst trained our policy ÉŒ0(si) to predict the expert
j in

action by minimizing the loss L between ÉŒ0(si
a supervised approach:

j) and ai

1 , a(i)

1 ), . . . ]

(12)

Li

j = (cid:107)ÉŒ0(si

j) Å| ai

j(cid:107)2, ÅÕ sj Å∏ |T i|, i Å∏ N

(13)

Subsequent policy updates are computed according to DAg-
ger [28].

We also implemented a method to increase the number
of expert demonstrations iteratively. For a given object,
every object transformation trajectory is specied by a 12-
dimensional vector containing the object starting and tar-
get position and orientation: (xi
obj,t). By
imitating the expert demonstration examples, we learn a
policy supported by these expert demonstrations. This control
embedding is able to interpolate between known trajectories

obj,s, xi

obj,s, qi

obj,t, qi

using a nearest neighbor policy in order to generate trajec-
tories for previously unseen transformations.

obj,t, qi

obj,s, xi

Based on the learned policy, we accumulate nearby trans-
formations. Let D = (xi
obj,s, qi
obj,t) represent
the transformations which our policy already knows. If an
interpolated trajectory transformation is nearby to one of the
transformations in D, we add the transformation to D, and
then continue to train our policy based on the transformation
demonstrations in D. Through this fashion, we kept growing
and maintaining a traversable graph of transforming the
object between various poses, similar to [35]. The learned
control policy could also be treated as an efcient initializa-
tion for a deep reinforcement learning approach or motion
planning for in-hand manipulations.

V. EXPERIMENTS

A. Simulation Experiments

Mujoco 2.0 was used to simulate both the handcrafted
control policy and the learned policy before transferring
them to the physical setup (see Fig. 6). By default, Mujoco
assumes a soft-contact model which leads to relatively slip-
pery contacts, but the solver parameters (solref and solimp)
were changed to have a harder contact model in order to
better model the friction between the object and the spherical
rollers. An elliptical friction cone was used along with the
Newton solver to evaluate these constraints.

The gripper was modeled in the software, and the fol-
lowing setup was specied. The base joints were position
controlled, and had their force output clamped; these settings
allowed the ngers to act compliantly and conform to an
objectÅfs shape as it was reoriented, and stabilized the grasp.
The pivots had a rotation range from [Å| ÉŒ
2 ] (with the zero
position corresponding with the roller axis aligned along the
length of the nger) in order to represent the limited range
in the physical setup due to motor wires. A pivot rotation
threshold of 3 per time step was used to prohibit quick
jumps between the two rotation limits in favor of smoother
motion that stabilized the simulation.

2 , ÉŒ

Sensors were placed at all nine joints to read joint posi-
tions and two sensors were placed on the object to obtain
its orientation (quaternion) and 3D position relative to the
global frame. The learned policy outputted values for all nine
joints. The handcrafted control policy only used the latter two

Fig. 6. A visualization of our simulation in Mujoco. In simulation the
gripper maintains the same physical design as the real gripper.

Fig. 7.
Top row: the cube being manipulated from a starting position
shown on the left. Bottom row: the novel object, a rectangular prism being
similarly manipulated

sensors and the base joint positions to calculate the output
velocities for the pivots and rollers at every time step.

Experiments were run by specifying a desired orientation
in angle-axis representation (cid:0)[x, y, z]T , É∆(cid:1). Most experi-
ments were carried out with a 6 cm cube with a mass of
32.4 grams that had a starting quaternion of q0 = [1, 0, 0, 0].
Object rotation angles were specied as at least 90 in the
majority of experiments to ensure that the rollers had to
traverse across the edges of the cube. Rotation axes ranged
from more vertical axes  which were generally easier for
the simulation to realize  to purely horizontal axes which
were more difcult.

B. Experimental Setup

The experimental setup included the grasper, an overhead
Intel Realsense D400series RGBD camera, and various 3D
printed objects including a cube, a cube with lleted edges,
and spheres of various masses and sizes. The handcrafted
control policy was able to be run both open-loop and closed-
loop. Since object orientation and position were used as input
to the control policy, only the cube was run in the closed-loop
operation with QR-tags on all 6 faces. Open-loop runs with
the spheres were used to qualitatively verify the handcrafted
control policy on the hardware.

At the start of each experiment the ngers would spread
out, allowing a human operator to hold the cube at approx-
imately the initial position specied for that particular trial.
The ngers would then move inwards and grasp the object.
At every time step the joint positions, object orientation, and
position would be read-in, from which the corresponding
joint output would be calculated and sent to the actuators.

C. Evaluation Metric

We adopted the orientation error metric suggested in [36]
which can be computed by using the sum of quaternions and
normalizing. Given the desired object orientation, qobj,d, and
the current object orientation, qobj,c, the error is calculated
as the following:

well as ne tuning of the contact model to more closely align
with observations of the physical system performance.

B. Untrained Target Pose Test Cases

For the difcult transformations (D1 Å® D6), the imitation
learning outperformed the handcrafted policy across all 6
trials by an average percent average error of 16.0%. Three of
these trials, D1 Å® D3, including target poses not trained in
simulation. Overall, the imitation learning achieved a percent
average error of 25.3%Å}4.2%, while the handcrafted control
policy achieved 41.3% Å} 5.8%. Again, the imitation learning
was able to demonstrate more stable trajectories despite the
sensor noise.

C. Novel Object Test Cases

Tests with the novel object (N1 Å® N2), a rectangular
prism, demonstrated similar results to the simple transforma-
tion case. The percent average errors were comparable be-
tween imitation learning (17.1%Å}3.9%) and the handcrafted
control policy (20.3% Å} 10.3%). However, the handcrafted
control policy had a much higher standard deviation.

D. Discussion

A potential explanation for the poorer performance of the
handcrafted control policy, especially for the difcult trans-
formations, was that it always generated linear trajectories
from the current position to the target. However, in many
cases, especially for rotation axes with dominating horizontal
components, taking a linear path can lead to the gripper
dropping the object. As the object is rotated closer to the
desired axis of rotation, one or more of the rollers can
roll onto a surface where the contact normal has a Å|ZG
component; the contact normal can overcome the frictional
support provided by another roller and result in dropping
the object. This problem can be mitigated by increasing
the positional error compensation in the closed-loop control
method in order to more actively control the object height.
Unfortunately, placing a greater emphasis on controlling
the object position was seen to cause previously successful
trajectories to fail. No combination of gains was seen to
work across all observed successes. On the other hand,
the imitation learning was better for a couple of potential
reasons: (1) the policy was learned from noisy sensor data
which increased its robustness, and (2) the discovery of safe
and repeatable trajectories.

VII. CONCLUSION AND FUTURE WORK

This paper presents Roller Grasper V2, a new design for
a grasper based on steerable spherical rollers located at the
nger tips. The hardware design choices and engineering
details were provided. A handcrafted control policy was
constructed that utilized the active surfaces of the rollers to
transform an object to an arbitrary target pose. This control
policy was used to generate expert trajectories in order to
develop an imitation learning based policy. This learned
policy was able to interpolate between learned trajectories in
order to successfully reach new targets; these new trajectories

Fig. 8. Orientation error for real world experiment results. The vertical axes
represents the orientation error dened in V-C. The horizontal lines repre-
sents various experiments. S D and N denote simple manipulations tasks,
difcult manipulation tasks and novel object transformations,respectively.
Notes: (1) The handcrafted control is designed to stop at a pre-dened
error threshold, therefore its orientation error is not meaningful to shown.
(2) Imitation learning policy in simulation is deterministic. (3) The novel
object transformation was only tested in real-world experiments, thus no
simulation results were available.

eÉ÷ = 100

min(||qobj,d Å| qobj,c||2, ||qobj,d + qobj,c||2)
Å„
2

(14)

VI. RESULTS AND ANALYSIS

The results of the experiments carried out on the hard-
ware are presented in Fig. 8. All reported percent average
errors were calculated according to (14). The experiments
were divided up as follows: simple transformations, which
had axes of rotations with strong ZG components, difcult
transformations, which had axes of rotations with strong x
and y components, and novel object transformations, which
consisted of transforming an elongated rectangular prism
with lleted edges (6 cm Å~ 6 cm Å~ 8 cm) (Fig. 7). The
reason transformations with strong ZG components for the
rotation axis were easier was due to the three-nger design
of the grasper, leading to a much better force closure around
XG Å| YG direction than the ZG direction. All experiments
specied the target as an axis and a 90 rotation about
that axis. This choice of a rotation angle is described in
Simulation Experiments.

A. Simple Cases & Sim-to-Real Transferring

The imitation learning method performed better for 3 out
of the 5 simple transformation cases (S1 Å® S5). The average
orientation error across these 5 trials shows that the imitation
learning method (15.3% Å} 2.2%) and the handcrafted control
policy (13.6% Å} 7.2%) are comparable when performing
object rotations about more vertical axes. However,
the
handcrafted control policy has more than 3 times the standard
deviation of the imitation learning.

As with any physical system there is a performance dif-
ference when compared to the simulation setup. Fortunately,
this difference was small with the imitation learningÅfs results
in simulation reporting an average error of 15.4%. We believe
this is due to inclusion of sensor noise in the simulation, as

were placed in a buffer of known trajectories in order to
develop a traversable graph. Experiments for the handcrafted
policy and the learned policy demonstrated the utility of this
graph for generating safe trajectories, as the learned policy
outperformed the handcrafted policy for all difcult target
poses. For simple cases, the two policies performed compa-
rably, except the learned policy had much lower variance
in the trajectory. Future work includes reducing the size
of the rollers to allow for manipulation of smaller objects,
developing a novel mechanism that solves the non-holonomic
constraints and incorporating tactile sensing on the rollers to
provide high-delity feedback.

REFERENCES
[1] S. Yuan, A. D. Epps, J. B. Nowak, and J. K. Salisbury, ÅgDesign of
a roller-based dexterous hand for object grasping and within-hand
manipulation,Åh in 2020 IEEE International Conference on Robotics
and Automation, May 2020.

[2] C. Piazza, G. Grioli, M. Catalano, and A. Bicchi, ÅgA century of robotic
hands,Åh Annual Review of Control, Robotics, and Autonomous Systems,
vol. 2, pp. 132, 2019.

[3] M. A. Diftler, J. S. Mehling, M. E. Abdallah, N. A. Radford, L. B.
Bridgwater, A. M. Sanders, R. S. Askew, D. M. Linn, J. D. Yamokoski,
F. A. Permenter, B. K. Hargrave, R. Platt, R. T. Savely, and R. O.
Ambrose, ÅgRobonaut 2 - the rst humanoid robot
in space,Åh in
2011 IEEE International Conference on Robotics and Automation,
pp. 21782183, May 2011.

[4] Shadow Robot Company, ÅgDesign of a dextrous hand for advanced
CLAWAR applications,Åh in Proceedings of the 6th International con-
ference on climbing and walking robots and the supporting technolo-
gies for mobile machines, pp. 691698, September 2003.

[5] M. Grebenstein, A. Albu-SchÅNaffer, T. Bahls, M. Chalon, O. Eiberger,
W. Friedl, R. Gruber, S. Haddadin, U. Hagn, R. Haslinger, H. HÅNoppner,
S. JÅNorg, M. Nickl, A. Nothhelfer, F. Petit, J. Reill, N. Seitz,
T. WimbÅNock, S. Wolf, T. WÅNusthoff, and G. Hirzinger, ÅgThe dlr hand
arm system,Åh in 2011 IEEE International Conference on Robotics and
Automation, pp. 31753182, May 2011.

[6] R. R. Ma and A. M. Dollar, ÅgAn underactuated hand for efcient
nger-gaiting-based dexterous manipulation,Åh in 2014 IEEE Inter-
national Conference on Robotics and Biomimetics (ROBIO 2014),
pp. 22142219, Dec 2014.

[7] A. M. Dollar and R. D. Howe, ÅgThe highly adaptive sdm hand: Design
and performance evaluation,Åh The International Journal of Robotics
Research, vol. 29, no. 5, pp. 585597, 2010.

[8] H. Stuart, S. Wang, O. Khatib, and M. R. Cutkosky, ÅgThe ocean
one hands: An adaptive design for robust marine manipulation,Åh The
International Journal of Robotics Research, vol. 36, no. 2, pp. 150
166, 2017.

[9] N. Rojas, R. R. Ma, and A. M. Dollar, ÅgThe gr2 gripper: an un-
deractuated hand for open-loop in-hand planar manipulation,Åh IEEE
Transactions on Robotics, vol. 32, no. 3, pp. 763770, 2016.

[10] N. Govindan and A. Thondiyath, ÅgDesign and Analysis of a Multi-
modal Grasper Having Shape Conformity and Within-Hand Manipu-
lation With Adjustable Contact Forces,Åh Journal of Mechanisms and
Robotics, vol. 11, 07 2019. 051012.

[11] V. Tincani, M. G. Catalano, E. Farnioli, M. Garabini, G. Grioli,
G. Fantoni, and A. Bicchi, ÅgVelvet ngers: A dexterous gripper
with active surfaces,Åh in 2012 IEEE/RSJ International Conference on
Intelligent Robots and Systems, pp. 12571263, Oct 2012.

[12] R. R. Ma and A. M. Dollar, ÅgIn-hand manipulation primitives for
a minimal, underactuated gripper with active surfaces,Åh in proceed-
ings of the ASME 2016 International Design Engineering Technical
Conferences (IDETC), Computers and Information in Engineering
Conference (CIE), p. V05AT07A072, 2016.

[13] V. Kumar, E. Todorov, and S. Levine, ÅgOptimal control with learned lo-
cal models: Application to dexterous manipulation,Åh in 2016 IEEE In-
ternational Conference on Robotics and Automation (ICRA), pp. 378
383, IEEE, 2016.

[14] O. M. Andrychowicz, B. Baker, M. Chociej, R. Jozefowicz, B. Mc-
Grew, J. Pachocki, A. Petron, M. Plappert, G. Powell, A. Ray, et al.,
ÅgLearning dexterous in-hand manipulation,Åh The International Journal
of Robotics Research, vol. 39, no. 1, pp. 320, 2020.

[15] N. Chavan-Dae, R. Holladay, and A. Rodriguez, ÅgIn-hand manipula-
tion via motion cones,Åh arXiv preprint arXiv:1810.00219, 2018.
[16] S. Cruciani, C. Smith, D. Kragic, and K. Hang, ÅgDexterous manipula-
tion graphs,Åh in 2018 IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS), pp. 20402047, IEEE, 2018.

[17] M. Liarokapis and A. M. Dollar, ÅgDeriving dexterous, in-hand ma-
nipulation primitives for adaptive robot hands,Åh in 2017 IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS),
pp. 19511958, IEEE, 2017.

[18] N. Rahman, L. Carbonari, M. DÅfImperio, C. Canali, D. G. Caldwell,
and F. Cannella, ÅgA dexterous gripper for in-hand manipulation,Åh in
2016 IEEE International Conference on Advanced Intelligent Mecha-
tronics (AIM), pp. 377382, July 2016.

[19] W. G. Bircher, A. M. Dollar, and N. Rojas, ÅgA two-ngered robot grip-
per with large object reorientation range,Åh in 2017 IEEE International
Conference on Robotics and Automation (ICRA), pp. 34533460, May
2017.

[20] Y. Karayiannidis, K. Pauwels, C. Smith, D. Kragic, et al., ÅgIn-hand
manipulation using gravity and controlled slip,Åh in 2015 IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS),
pp. 56365641, IEEE, 2015.

[21] D. L. Brock, ÅgEnhancing the dexterity of a robot hand using controlled
slip,Åh in Proceedings. 1988 IEEE International Conference on Robotics
and Automation, pp. 249251, IEEE, 1988.

[22] N. C. Dae, A. Rodriguez, R. Paolini, B. Tang, S. S. Srinivasa,
M. Erdmann, M. T. Mason, I. Lundberg, H. Staab, and T. Fuhlbrigge,
ÅgExtrinsic dexterity: In-hand manipulation with external forces,Åh in
2014 IEEE International Conference on Robotics and Automation
(ICRA), pp. 15781585, IEEE, 2014.

[23] J. Shi, J. Z. Woodruff, P. B. Umbanhowar, and K. M. Lynch, ÅgDynamic
in-hand sliding manipulation,Åh IEEE Transactions on Robotics, vol. 33,
no. 4, pp. 778795, 2017.

[24] N. Chavan-Dae and A. Rodriguez, ÅgPrehensile pushing: In-hand
manipulation with push-primitives,Åh in 2015 IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS), pp. 62156222,
IEEE, 2015.

[25] C. Eppner, R. Deimel, J. Alvarez-Ruiz, M. Maertens, and O. Brock,
ÅgExploitation of environmental constraints in human and robotic
grasping,Åh The International Journal of Robotics Research, vol. 34,
no. 7, pp. 10211038, 2015.

[26] H. Van Hoof, T. Hermans, G. Neumann, and J. Peters, ÅgLearning robot
in-hand manipulation with tactile features,Åh in 2015 IEEE-RAS 15th
International Conference on Humanoid Robots (Humanoids), pp. 121
127, IEEE, 2015.

[27] D. A. Pomerleau, ÅgAlvinn: An autonomous land vehicle in a neural
information processing systems,

network,Åh in Advances in neural
pp. 305313, 1989.

[28] S. Ross, G. Gordon, and D. Bagnell, ÅgA reduction of imitation learning
and structured prediction to no-regret online learning,Åh in Proceedings
of the fourteenth international conference on articial intelligence and
statistics, pp. 627635, 2011.

[29] A. Y. Ng, S. J. Russell, et al., ÅgAlgorithms for inverse reinforcement

learning.,Åh

[30] L. Shao, T. Migimatsu, Q. Zhang, K. Yang, and J. Bohg, ÅgCon-
cept2robot: Learning manipulation concepts from instructions and
human demonstrations,Åh in Proceedings of Robotics: Science and
Systems (RSS), July 2020.

[31] A. Billard, Y. Epars, S. Calinon, S. Schaal, and G. Cheng, ÅgDiscov-
ering optimal imitation strategies,Åh Robotics and autonomous systems,
vol. 47, no. 2-3, pp. 6977, 2004.

[32] P. Pastor, H. Hoffmann, T. Asfour, and S. Schaal, ÅgLearning and
generalization of motor skills by learning from demonstration,Åh in 2009
IEEE International Conference on Robotics and Automation, pp. 763
768, IEEE, 2009.

[33] M. R. Cutkosky and I. Kao, ÅgComputing and controlling compliance
of a robotic hand,Åh IEEE transactions on robotics and automation,
vol. 5, no. 2, pp. 151165, 1989.

[34] M. T. Mason and J. K. Salisbury Jr, ÅgRobot hands and the mechanics

of manipulation,Åh 1985.

[35] S. Cruciani, K. Hang, C. Smith, and D. Kragic, ÅgDual-arm in-hand
manipulation and regrasping using dexterous manipulation graphs,Åh
arXiv preprint arXiv:1904.11382, 2019.

[36] D. Q. Huynh, ÅgMetrics for 3d rotations: Comparison and analysis,Åh
Journal of Mathematical Imaging and Vision, vol. 35, no. 2, pp. 155
164, 2009.


